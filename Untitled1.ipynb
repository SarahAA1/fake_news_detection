{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('mediaeval-2015-testset.txt',sep='\\t')\n",
    "train = pd.read_csv('mediaeval-2015-trainingset.txt',sep='\\t')\n",
    "test = test.set_index('tweetId', drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data:  (14277, 7)\n",
      "\n",
      " \n",
      " TRAIN \n",
      "               tweetId                                          tweetText  \\\n",
      "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
      "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
      "2  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
      "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
      "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
      "\n",
      "      userId      imageId(s)        username                       timestamp  \\\n",
      "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
      "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
      "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
      "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
      "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
      "\n",
      "  label  \n",
      "0  fake  \n",
      "1  fake  \n",
      "2  fake  \n",
      "3  fake  \n",
      "4  fake  \n",
      "\n",
      " \n",
      " TEST \n",
      "                                                             tweetText  \\\n",
      "tweetId                                                                 \n",
      "578854927457349632  kereeen RT @Shyman33: Eclipse from ISS.... htt...   \n",
      "578874632670953472  Absolutely beautiful! RT @Shyman33: Eclipse fr...   \n",
      "578891261353984000  “@Shyman33: Eclipse from ISS.... http://t.co/C...   \n",
      "578846612312748032        Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
      "578975333841551360  @ebonfigli: Éclipse vue de l'ISS... Autre chos...   \n",
      "\n",
      "                        userId   imageId(s)         username  \\\n",
      "tweetId                                                        \n",
      "578854927457349632    70824972  eclipse_01            peay_s   \n",
      "578874632670953472   344707006  eclipse_01   JaredUcanChange   \n",
      "578891261353984000   224839607  eclipse_01          tpjp1231   \n",
      "578846612312748032   134543073  eclipse_01          Shyman33   \n",
      "578975333841551360  1150728872   eclipse_01       Epimethee_   \n",
      "\n",
      "                                         timestamp label  \n",
      "tweetId                                                   \n",
      "578854927457349632  Fri Mar 20 09:45:43 +0000 2015  fake  \n",
      "578874632670953472  Fri Mar 20 11:04:02 +0000 2015  fake  \n",
      "578891261353984000  Fri Mar 20 12:10:06 +0000 2015  fake  \n",
      "578846612312748032  Fri Mar 20 09:12:41 +0000 2015  fake  \n",
      "578975333841551360  Fri Mar 20 17:44:11 +0000 2015  fake  \n",
      "\n",
      " \n",
      "Number of Null values in Train Set:  0\n",
      "Number of Null values in Test Set:  0\n"
     ]
    }
   ],
   "source": [
    "# Counting number of rows and columns in the data\n",
    "print('Shape of Training Data: ', train.shape)\n",
    "\n",
    "# Gettiing a hang of the data in each column and their names\n",
    "print('\\n \\n TRAIN \\n', train.head())\n",
    "print('\\n \\n TEST \\n', test.head())\n",
    "\n",
    "# Looking for any places where training data has NaN values\n",
    "print('\\n \\nNumber of Null values in Train Set: ', train['tweetText'].isna().sum())\n",
    "print('Number of Null values in Test Set: ', test['tweetText'].isna().sum())\n",
    "\n",
    "# Dropping all rows where text column is NaN\n",
    "train.dropna(axis=0, how=\"any\", thresh=None, subset=['tweetText'], inplace=True)\n",
    "test = test.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Length:  26 \n",
      "Maximum Length:  7125 \n",
      "Average Length:  92\n"
     ]
    }
   ],
   "source": [
    "# Checking length of each article\n",
    "length = []\n",
    "[length.append(len(str(text))) for text in train['tweetText']]\n",
    "train['length'] = length\n",
    "print('Minimum Length: ', min(train['length']), '\\nMaximum Length: ', max(train['length']), '\\nAverage Length: ', round(sum(train['length'])/len(train['length'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with more than 250 words:  135\n",
      "486       142\n",
      "571      1658\n",
      "773       142\n",
      "1801      142\n",
      "1803     4786\n",
      "         ... \n",
      "14254     146\n",
      "14255     145\n",
      "14258     144\n",
      "14266     144\n",
      "14268     149\n",
      "Name: length, Length: 135, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Minimum length is 1. We need to spot some outliers and get rid of them. Counting how many outliers are there\n",
    "print('Number of articles with more than 250 words: ', len(train[train['length'] >140]))\n",
    "# Skimming through such short texts just to be sure\n",
    "print(train['length'][train['length'] > 140] )\n",
    "\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Length:  26 \n",
      "Maximum Length:  140 \n",
      "Average Length:  88\n"
     ]
    }
   ],
   "source": [
    "# Removing outliers, it will reduce overfitting\n",
    "train = train.drop(train['tweetText'][train['length'] > 140].index, axis = 0)\n",
    "print('Minimum Length: ', min(train['length']), '\\nMaximum Length: ', max(train['length']), '\\nAverage Length: ', round(sum(train['length'])/len(train['length'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secluding labels in a new pandas dataframe for supervised learning\n",
    "train_labels = train['label']\n",
    "# Splitting data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(train['tweetText'], train_labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Term Frequency - Inverse Document Frequency Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n",
    "# Fit and transform training set and transform test set\n",
    "tfidf_train = tfidf.fit_transform(x_train) \n",
    "tfidf_test = tfidf.transform(x_test)\n",
    "tfidf_test_final = tfidf.transform(test['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.41%\n"
     ]
    }
   ],
   "source": [
    "# Setting up Passive Aggressive Classifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 50)\n",
    "# Fitting on the training set\n",
    "pac.fit(tfidf_train, y_train)\n",
    "# Predicting on the test set\n",
    "y_pred = pac.predict(tfidf_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetText</th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>fake</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>fake</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>fake</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>fake</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>fake</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweetText        username label  \\\n",
       "0  ¿Se acuerdan de la película: “El día después d...         iAnnieM  fake   \n",
       "1  @milenagimon: Miren a Sandy en NY!  Tremenda i...  CarlosVerareal  fake   \n",
       "2  Buena la foto del Huracán Sandy, me recuerda a...     LucasPalape  fake   \n",
       "3     Scary shit #hurricane #NY http://t.co/e4JLBUfH     Haaaaarryyy  fake   \n",
       "4  My fave place in the world #nyc #hurricane #sa...  princess__natt  fake   \n",
       "\n",
       "   length  \n",
       "0     134  \n",
       "1     133  \n",
       "2     116  \n",
       "3      46  \n",
       "4      89  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop([\"tweetId\"],axis=1,inplace=True)\n",
    "data.drop([\"userId\"],axis=1,inplace=True)\n",
    "\n",
    "data.drop([\"imageId(s)\"],axis=1,inplace=True)\n",
    "\n",
    "data.drop([\"timestamp\"],axis=1,inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14142, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "\n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "\n",
    "data['Lang'] = data['tweetText'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = data[ (data['Lang'] != \"en\")].index\n",
    "data.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secluding labels in a new pandas dataframe for supervised learning\n",
    "train_labels = data['label']\n",
    "# Splitting data into training and testing sets\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(data['tweetText'], train_labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Term Frequency - Inverse Document Frequency Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n",
    "# Fit and transform training set and transform test set\n",
    "tfidf_train1 = tfidf.fit_transform(x_train1) \n",
    "tfidf_test1 = tfidf.transform(x_test1)\n",
    "tfidf_test_final1 = tfidf.transform(test['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.32%\n"
     ]
    }
   ],
   "source": [
    "# Setting up Passive Aggressive Classifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 50)\n",
    "\n",
    "# Fitting on the training set\n",
    "pac.fit(tfidf_train1, y_train1)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred1 = pac.predict(tfidf_test1)\n",
    "score = accuracy_score(y_test1, y_pred1)\n",
    "print(f'Accuracy: {round(score * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels\n",
    "data.loc[(data['label'] == 0) , ['label']] = 'fake'\n",
    "data.loc[(data['label'] == 1) , ['label']] = 'real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].replace(\"humor\", \"fake\",inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    fake\n",
       "4    fake\n",
       "5    fake\n",
       "6    fake\n",
       "7    fake\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the labels\n",
    "labels = data.label\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['tweetText'].values.astype('str'), labels, test_size=0.2, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & transform train set, transform test set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test=tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(max_iter=50)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the PassiveAggressiveClassifier and fit training sets\n",
    "pa_classifier=PassiveAggressiveClassifier(max_iter=50)\n",
    "pa_classifier.fit(tfidf_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.14%\n"
     ]
    }
   ],
   "source": [
    "# Predict and calculate accuracy\n",
    "y_pred=pa_classifier.predict(tfidf_test)\n",
    "score=accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1285,   67],\n",
       "       [ 125,  691]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build confusion matrix\n",
    "confusion_matrix(y_test,y_pred, labels=['fake','real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 89.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 90.91%\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "# Fitting the model\n",
    "model = pipe.fit(x_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 77.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(x_train, y_train)\n",
    "# Accuracy\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
